{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# our mlp class definition\n",
    "\n",
    "# we want to implement:\n",
    "#1. backpropagation\n",
    "#2. gradient descent\n",
    "#3. train\n",
    "#4. train our network with post operative dataset\n",
    "\n",
    "\n",
    "\n",
    "class Multilayer_Perceptron(object):\n",
    "    \n",
    "    # constructor of the class\n",
    "    # we have 8 features(inputs) and 3 classes\n",
    "    def __init__(self, num_inputs=8, hidden_layers=[5,4,3,3,3], num_outputs=3):\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_outputs = num_outputs\n",
    "        \n",
    "        # representation of layers\n",
    "        \n",
    "        layers = [num_inputs] + hidden_layers + [num_outputs]\n",
    "        \n",
    "        \n",
    "        # initialization of weights with random small values\n",
    "        weights = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            w = np.random.rand(layers[i] , layers[i+1])\n",
    "            weights.append(w)\n",
    "            \n",
    "        self.weights = weights\n",
    "        \n",
    "        \n",
    "        # defining activations\n",
    "        # we explained about this completely in report\n",
    "        activations = []\n",
    "        for i in range(len(layers)):\n",
    "            a = np.zeros(layers[i])\n",
    "            activations.append(a)\n",
    "            \n",
    "        self.activations = activations\n",
    "        \n",
    "        \n",
    "        \n",
    "        derivatives = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            derivative = np.zeros((layers[i] , layers[i+1]))\n",
    "            derivatives.append(derivative)\n",
    "            \n",
    "        self.derivatives = derivatives\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "    # forward propagation method\n",
    "    def forward_propagation(self, inputs):\n",
    "        \n",
    "        \n",
    "        activations = inputs\n",
    "        \n",
    "\n",
    "        self.activations[0] = inputs\n",
    "        \n",
    "        for i, w in enumerate(self.weights):\n",
    "            \n",
    "            net = np.dot(activations, w)\n",
    "            \n",
    "            activations = self._sigmoid(net)\n",
    "            self.activations[i+1] = activations\n",
    "            \n",
    "        return activations\n",
    "    \n",
    "    \n",
    "    # defining our activation function\n",
    "    def _sigmoid(self, x):\n",
    "        \n",
    "        result = 1.0/(1 + np.exp(-x))\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def back_propagation(self, error):\n",
    "        \n",
    "        # for weights between output layer and previous layer we had:\n",
    "        # dE/dWi = (y - a_[i+1]) * (s'(v_[i+1])) * (a_i)\n",
    "        # dE/dWi = ###########delta############# * (a_i)\n",
    "        \n",
    "        # s'(v_[i+1]) = s(v_[i+1]) * (1 - s(v_[i+1])) -----> sigmoid derivative\n",
    "        # s(v_[i+1]) = a_[i+1]\n",
    "        \n",
    "        # with assumptions:\n",
    "        # v = net input of neurons\n",
    "        # s is sigmoid activation function\n",
    "        # y is desired output\n",
    "        \n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "            \n",
    "            activations = self.activations[i+1]\n",
    "            delta = error * self._sigmoid_derivative(activations)\n",
    "            delta_reshaped = delta.reshape(delta.shape[0], -1).T\n",
    "            \n",
    "            current_activations = self.activations[i]\n",
    "            current_activations_reshaped = current_activations.reshape(current_activations.shape[0], -1)\n",
    "            \n",
    "            self.derivatives[i] = np.dot(current_activations_reshaped, delta_reshaped)\n",
    "            error = np.dot(delta, self.weights[i].T)\n",
    "            \n",
    "        # we return error backpropagated all the way back to the input layer\n",
    "        return error\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    def _sigmoid_derivative(self, x):\n",
    "        \n",
    "        return x * (1.0 - x)\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            weights = self.weights[i]\n",
    "            derivatives = self.derivatives[i]\n",
    "            weights += derivatives * learning_rate\n",
    "            \n",
    "    def train(self, inputs, targets, epochs, learning_rate):\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            sum_error = 0\n",
    "            \n",
    "            for input_, target in zip(inputs, targets):\n",
    "                \n",
    "                # forward propagation\n",
    "                output = self.forward_propagation(input_)\n",
    "                \n",
    "                # calculating error\n",
    "                error = target - output\n",
    "                \n",
    "                # back propagation\n",
    "                self.back_propagation(error)\n",
    "                \n",
    "                # applying gradient descent\n",
    "                self.gradient_descent(learning_rate)\n",
    "                \n",
    "                sum_error += self._mse(target, output)\n",
    "                \n",
    "                \n",
    "            # printing error in each epoch\n",
    "            print(\"Error: {} at epoch {}\".format(sum_error / len(inputs), i+1))\n",
    "            \n",
    "            \n",
    "            \n",
    "    def modified_one_per_epoch_train(self, inputs, targets, epochs, learning_rate):\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            sum_error = 0\n",
    "            \n",
    "            for input_, target in zip(inputs, targets):\n",
    "                \n",
    "                # forward propagation\n",
    "                output = self.forward_propagation(input_)\n",
    "                \n",
    "                # calculating error\n",
    "                error = target - output\n",
    "                \n",
    "                # back propagation\n",
    "                self.back_propagation(error)\n",
    "                \n",
    "                # applying gradient descent\n",
    "                #self.gradient_descent(learning_rate)\n",
    "                \n",
    "                sum_error += self._mse(target, output)\n",
    "                \n",
    "            self.gradient_descent(learning_rate)\n",
    "            \n",
    "            # printing error in each epoch\n",
    "            print(\"Error: {} at epoch {}\".format(sum_error / len(inputs), i+1))\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "    def modified_two_per_epoch_train(self, inputs, targets, epochs, learning_rate):\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            sum_error = 0\n",
    "            \n",
    "            for input_, target in zip(inputs, targets):\n",
    "                \n",
    "                # forward propagation\n",
    "                output = self.forward_propagation(input_)\n",
    "                \n",
    "                # calculating error\n",
    "                error = target - output\n",
    "                \n",
    "                # back propagation\n",
    "                self.back_propagation(error)\n",
    "                \n",
    "                # applying gradient descent\n",
    "                #self.gradient_descent(learning_rate)\n",
    "                \n",
    "                sum_error += self._mse(target, output)\n",
    "                \n",
    "            self.gradient_descent(learning_rate)\n",
    "            self.gradient_descent(learning_rate)\n",
    "            \n",
    "            # printing error in each epoch\n",
    "            print(\"Error: {} at epoch {}\".format(sum_error / len(inputs), i+1))\n",
    "            \n",
    "            \n",
    "    def modified_three_per_epoch_train(self, inputs, targets, epochs, learning_rate):\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            sum_error = 0\n",
    "            \n",
    "            for input_, target in zip(inputs, targets):\n",
    "                \n",
    "                # forward propagation\n",
    "                output = self.forward_propagation(input_)\n",
    "                \n",
    "                # calculating error\n",
    "                error = target - output\n",
    "                \n",
    "                # back propagation\n",
    "                self.back_propagation(error)\n",
    "                \n",
    "                # applying gradient descent\n",
    "                #self.gradient_descent(learning_rate)\n",
    "                \n",
    "                sum_error += self._mse(target, output)\n",
    "                \n",
    "            self.gradient_descent(learning_rate)\n",
    "            self.gradient_descent(learning_rate)\n",
    "            self.gradient_descent(learning_rate)\n",
    "            \n",
    "            \n",
    "            # printing error in each epoch\n",
    "            print(\"Error: {} at epoch {}\".format(sum_error / len(inputs), i+1))\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "    def _mse(self, target, output):\n",
    "        \n",
    "        result = np.average((target - output)**2)\n",
    "        return result\n",
    "      \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.44855224855316245 at epoch 1\n",
      "Error: 0.4377585690794656 at epoch 2\n",
      "Error: 0.4264779014911371 at epoch 3\n",
      "Error: 0.4147612365712031 at epoch 4\n",
      "Error: 0.40267310961259495 at epoch 5\n",
      "Error: 0.3902900952228921 at epoch 6\n",
      "Error: 0.37769846141052066 at epoch 7\n",
      "Error: 0.36499126311405006 at epoch 8\n",
      "Error: 0.3522652420281898 at epoch 9\n",
      "Error: 0.33961787741266 at epoch 10\n",
      "Error: 0.32714480538871604 at epoch 11\n",
      "Error: 0.3149376453600702 at epoch 12\n",
      "Error: 0.30308212181063754 at epoch 13\n",
      "Error: 0.2916563143459718 at epoch 14\n",
      "Error: 0.2807289268122147 at epoch 15\n",
      "Error: 0.270357599990465 at epoch 16\n",
      "Error: 0.26058743035406307 at epoch 17\n",
      "Error: 0.2514499323568838 at epoch 18\n",
      "Error: 0.24296266172078798 at epoch 19\n",
      "Error: 0.23512961514454803 at epoch 20\n",
      "Error: 0.22794238223534444 at epoch 21\n",
      "Error: 0.22138189921506096 at epoch 22\n",
      "Error: 0.2154205766032178 at epoch 23\n",
      "Error: 0.21002455540238665 at epoch 24\n",
      "Error: 0.20515587792716014 at epoch 25\n",
      "Error: 0.2007744192823453 at epoch 26\n",
      "Error: 0.19683949235003506 at epoch 27\n",
      "Error: 0.19331109781281564 at epoch 28\n",
      "Error: 0.1901508337685066 at epoch 29\n",
      "Error: 0.18732250571658404 at epoch 30\n",
      "Error: 0.18479248970206066 at epoch 31\n",
      "Error: 0.18252990321853374 at epoch 32\n",
      "Error: 0.18050663405636486 at epoch 33\n",
      "Error: 0.17869726978912356 at epoch 34\n",
      "Error: 0.17707896219211328 at epoch 35\n",
      "Error: 0.17563125288125903 at epoch 36\n",
      "Error: 0.17433587950050977 at epoch 37\n",
      "Error: 0.17317657609613663 at epoch 38\n",
      "Error: 0.17213887687383683 at epoch 39\n",
      "Error: 0.1712099291927246 at epoch 40\n",
      "Error: 0.17037831921766938 at epoch 41\n",
      "Error: 0.16963391193582347 at epoch 42\n",
      "Error: 0.16896770607339937 at epoch 43\n",
      "Error: 0.16837170368267101 at epoch 44\n",
      "Error: 0.16783879369402127 at epoch 45\n",
      "Error: 0.16736264845732093 at epoch 46\n",
      "Error: 0.16693763216659246 at epoch 47\n",
      "Error: 0.16655872002474434 at epoch 48\n",
      "Error: 0.16622142702768056 at epoch 49\n",
      "Error: 0.16592174530562648 at epoch 50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset =  pd.read_csv('post-operative.data', sep=\",\")\n",
    "#print(dataset)\n",
    "\n",
    "data_list = dataset.values.tolist()\n",
    "\n",
    "\n",
    "#print(data_list[0])\n",
    "input_list = [None] * len(data_list)\n",
    "target_list = [None] * len(data_list)\n",
    "\n",
    "\n",
    "#print(len(target_list))\n",
    "\n",
    "for i in range(len(data_list)):\n",
    "    input_list[i] = data_list[i][0:8] # because inputs are from index 0 to 7\n",
    "    target_list[i] = data_list[i][8] # because target(class) value is in index 8\n",
    "        \n",
    "        \n",
    "# now we want to encode ordinal features of input and target lists to numerical\n",
    "\n",
    "for i in range(len(input_list)):\n",
    "    for j in range(len(input_list[i])):\n",
    "        \n",
    "        if input_list[i][j] == \"low\" or input_list[i][j] == \"poor\" or input_list[i][j] == \"unstable\":\n",
    "            input_list[i][j] = 5\n",
    "        elif input_list[i][j] == \"mid\" or input_list[i][j] == \"fair\" or input_list[i][j] == \"mod-stable\":\n",
    "            input_list[i][j] = 10\n",
    "        elif input_list[i][j] == \"high\" or input_list[i][j] == \"good\" or input_list[i][j] == \"stable\":\n",
    "            input_list[i][j] = 15\n",
    "        elif input_list[i][j] == \"excellent\":\n",
    "            input_list[i][j] = 20\n",
    "        elif input_list[i][j] == \"?\" : # missing values == \"?\"\n",
    "            input_list[i][j] = 0\n",
    "    \n",
    "    \n",
    "encoded_target_list = [None] * len(target_list)\n",
    "\n",
    "for i in range(len(target_list)):\n",
    "    if target_list[i] == \"I\":\n",
    "        encoded_target_list[i] = [1,0,0]\n",
    "    elif target_list[i] == \"S\":\n",
    "        encoded_target_list[i] = [0,1,0]\n",
    "    else:\n",
    "        encoded_target_list[i] = [0,0,1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#print(encoded_target_list)\n",
    "#print(input_list)\n",
    "\n",
    "\n",
    "\n",
    "# converting to numpy arrays\n",
    "input_list = np.array(input_list)\n",
    "encoded_target_list = np.array(encoded_target_list)\n",
    "\n",
    "# so untill now we have inputs and targets in \"input_list\" and \"encoded_target_list\" respectively, as numpy arrays.\n",
    "\n",
    "    \n",
    "#print(input_list)\n",
    "\n",
    "  \n",
    "input_list = input_list.astype(float)\n",
    "\n",
    "encoded_target_list = encoded_target_list.astype(float)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    # create a multilayer perceptron (instance of our class)\n",
    "    # read post operative dataset\n",
    "    # forward propagation\n",
    "    # back propagation\n",
    "    \n",
    "    # creating our network\n",
    "    multilayer_perceptron = Multilayer_Perceptron(8, [6,5,4,3,3], 3)\n",
    "    \n",
    "    \n",
    "    # now train our neural net\n",
    "    #multilayer_perceptron.train(input_list, encoded_target_list, 50, 0.1)\n",
    "    \n",
    "    multilayer_perceptron.modified_three_per_epoch_train(input_list, encoded_target_list, 50, 0.1)\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None]\n",
      "[[1, 2, 3], None, None]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "k = [\"low\" , \"mid\" , \"high\"]\n",
    "\n",
    "h = [[1,3],[6,3]]\n",
    "\n",
    "print(len(input_list[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'A', 'A ', 'A', 'S', 'S', 'S', 'S', 'S', 'A', 'A', 'A', 'A', 'S', 'A', 'A', 'A', 'A', 'A', 'A', 'S', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'S', 'A', 'A', 'A', 'A', 'A', 'A', 'S', 'S', 'S', 'A', 'A', 'S', 'S', 'S', 'A', 'S', 'I', 'A', 'A', 'A', 'A', 'A', 'A', 'S', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'S', 'A', 'S', 'A', 'A', 'A', 'A', 'A', 'A', 'S', 'A', 'A', 'A', 'A', 'A', 'S', 'A', 'A', 'S', 'I', 'A', 'A', 'A', 'S', 'A', 'A', 'S', 'A']\n"
     ]
    }
   ],
   "source": [
    "['10' '15' '20' '15' '15' '15' '15' '10']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
